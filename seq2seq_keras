# -*- coding: utf-8 -*-
"""
Created on Sun Mar 24 13:02:46 2019

@author: Vishal
"""

import nltk 
import numpy as np
#import tensorflow as tf
from collections import Counter
import re

#1
#Parameters
batch_size=128
num_epochs=10
max_input_seq_length=20
max_output_seq_length=20
max_vocab_size=100
hidden_units=256
TENSORBOARD = 'TensorBoard/'
WEIGHT_FILE_PATH = 'word-weights.h5'

#The input and the targeted list
input_texts = []
target_texts = []

#2
#opening the dataset and then spiliting the dataset into different lines
with open('cornell movie-dialogs corpus\movie_lines.txt','r',encoding='latin-1') as f:
    df=f.read()
    rows=df.split('\n')
    lines=[row.split(' +++$+++ ')[-1] for row in rows]
    
input_counter = Counter()
target_counter = Counter()    

'''
Dividing the lines list into two parts i.e input text and target_text
input text= first word 
target text= next word
and also including start and end to the target text list 
'''
prev_words=[]
for line in lines:
    next_word=[w.lower() for w in nltk.word_tokenize(line)]
    if len(next_word) > max_output_seq_length:
        next_word=next_word[:max_output_seq_length]
        
    if len(prev_words)>0:
        input_texts.append(prev_words)
        
        for w in prev_words:
            input_counter[w]+=1
            
        target_word=next_word[:]
        target_word.insert(0,'START')
        target_word.append('END')
        
        for w in target_word:
            target_counter[w]+=1
        target_texts.append(target_word)
        
    prev_words=next_word    
   
#3   
# Doing a first cleaning of the texts
def clean_text(text):
    text=' '.join(text)
    text = text.lower()
    text = re.sub(r"i'm", "i am", text)
    text = re.sub(r"he's", "he is", text)
    text = re.sub(r"she's", "she is", text)
    text = re.sub(r"that's", "that is", text)
    text = re.sub(r"what's", "what is", text)
    text = re.sub(r"where's", "where is", text)
    text = re.sub(r"\'ll", " will", text)
    text = re.sub(r"\'ve", " have", text)
    text = re.sub(r"\'re", " are", text)
    text = re.sub(r"\'d", " would", text)
    text = re.sub(r"won't", "will not", text)
    text = re.sub(r"can't", "cannot", text)
    text = re.sub(r"[-()\"#/@;:<>{}+=~|.?,!]", "", text)
    return text

# Cleaning the input
clean_input = []
for question in input_texts:
    clean_input.append(clean_text(question))


# Cleaning the target
clean_target = []
for ans in target_texts:
    clean_target.append(clean_text(ans))  
    
#4 
'''
Encoding the data
Using the most_common function of the counter class to get the 20 most common
and then giving them indexes+2 in the input text and index +1 in the target
text 
'''
input_word2dict={}
target_word2dict={}

for idx,word in enumerate(input_counter.most_common(max_vocab_size)):
    input_word2dict[word[0]]=idx+2
    
for idx,word in enumerate(target_counter.most_common(max_vocab_size)):
    target_word2dict[word[0]]=idx+1

input_word2dict['PAD'] = 0
input_word2dict['UNK'] = 1
target_word2dict['UNK'] = 0
 
#revrersing the key as value and value as the key as required
input_dict2word=dict([(idx,word) for word,idx in input_word2dict.items()])
target_dict2word=dict([(word,idx) for idx,word in target_word2dict.items()])


#5
#defining the number of tokens in the encoder and the decoder
num_encoder_tokens=len(input_dict2word)
num_decoder_tokens=len(target_dict2word)

#Defining the sequence length of the encoder and the decoder
encoder_max_seq_length = 0
decoder_max_seq_length = 0

#6
'''
loop to combine the input text and target text as a single input which will
then be passed to the encoder.
and also getting the maximum length of the encoder input text for the encoder
and the maximum length of target word for the decoder
'''
encoder_input_data = []
for input_words, target_words in zip(clean_input, clean_target):
    encoder_input_wids = []
    for w in input_words:
        w2idx = 1
        if w in input_word2dict:
            w2idx = input_word2dict[w]
        encoder_input_wids.append(w2idx)    
    encoder_input_data.append(encoder_input_wids)
    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)
    decoder_max_seq_length = max(len(target_words), decoder_max_seq_length)


#7
def generate_batch(input_data, output_text_data):
    num_batches = len(input_data) //batch_size
    while True:
        for batchIdx in range(0, num_batches) :
            start = batchIdx * batch_size
            end = (batchIdx + 1) * batch_size
            encoder_input_data_batch = pad_sequences(input_data[start:end], encoder_max_seq_length)
            decoder_target_data_batch = np.zeros(shape=(batch_size, decoder_max_seq_length, num_decoder_tokens))
            decoder_input_data_batch = np.zeros(shape=(batch_size, decoder_max_seq_length, num_decoder_tokens))
            for lineIdx, target_words in enumerate(output_text_data[start:end]):
                for idx, w in enumerate(target_words):
                    w2idx = 0
                    if w in target_word2dict:
                        w2idx = target_word2dict[w]
                    decoder_input_data_batch[lineIdx, idx, w2idx] = 1
                    if idx > 0:
                        decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1
            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch
   


#Importing all the  libraries that is required in order to implement Seq2seq Model 
#using keras
from keras.models import Model
from keras.layers import Input,Embedding,Dense
from keras.layers.recurrent import LSTM
from keras.preprocessing.sequence import pad_sequences
from keras.callbacks import ModelCheckpoint,TensorBoard


#Encoder
encoder_input=Input(shape=(None,),name='encoder_input')

encoder_embedding=Embedding(input_dim=num_encoder_tokens,
                            output_dim=hidden_units,
                            input_length=encoder_max_seq_length,
                            name='encoder_embedding')

encoder_lstm=LSTM(units=hidden_units,return_state=True,name='enocder_lstm')
encoder_outputs,encoder_state_h,encoder_state_c=encoder_lstm(encoder_embedding(encoder_input))
encoder_states=[encoder_state_h,encoder_state_c]


#Decoder

decoder_input=Input(shape=(None,num_decoder_tokens),name='decoder_input')

decoder_lstm=LSTM(units=hidden_units,return_state=True,return_sequences=True,name='decoder_lstm')

decoder_outputs,decoder_state_h,decoder_state_c=decoder_lstm(decoder_input,initial_state=encoder_states)

decoder_dense=Dense(units=num_decoder_tokens,activation='softmax',name='decoder_dense')

decoder_outputs=decoder_dense(decoder_outputs)

model=Model([encoder_input,decoder_input],decoder_outputs)

model.compile(loss='binary_crossentropy',optimizer='adam')

#8
#Train_test_split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(encoder_input_data, clean_target, test_size=0.2, random_state=42)


train_gen = generate_batch(X_train, y_train)
test_gen = generate_batch(X_test, y_test)

#number of batches 
# // - to get an integer value instead of a float value 
train_num_batches = len(X_train) // batch_size
test_num_batches = len(X_test) // batch_size

checkpoint = ModelCheckpoint(filepath=WEIGHT_FILE_PATH, save_best_only=True)
tbCallBack = TensorBoard(log_dir=TENSORBOARD, histogram_freq=0, write_graph=True, write_images=True)


model.fit_generator(generator=train_gen,
                    steps_per_epoch=train_num_batches,
                    epochs=num_epochs,
                    verbose=1,
                    validation_data=test_gen,
                    validation_steps=test_num_batches,
                    callbacks=[checkpoint,tbCallBack])

model.save_weights(WEIGHT_FILE_PATH)

class ChatBot():
    """
    This is ChatBot class it takes weights for the Neural Network, compliling model
    and returns prediction in responce to input text
    """
    def __init__(self):
        
        self.ultimate_question = 'Answer to the Ultimate Question of Life, the Universe, and Everything'
    #optinal
        encoder_inputs = Input(shape=(None, ), name='encoder_inputs')
        encoder_embedding = Embedding(input_dim=num_encoder_tokens, output_dim=hidden_units,input_length=encoder_max_seq_length, name='encoder_embedding')
        encoder_lstm = LSTM(units=hidden_units, return_state=True, name="encoder_lstm")
        encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_inputs))
        encoder_states = [encoder_state_h, encoder_state_c]

        decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_inputs')
        decoder_lstm = LSTM(units=hidden_units, return_sequences=True, return_state=True, name='decoder_lstm')
        decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
        decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')
        decoder_outputs = decoder_dense(decoder_outputs)

        self.model = Model([encoder_input, decoder_inputs], decoder_outputs)
        self.model.load_weights('word-weights.h5')
        self.model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
    #optinal
    
        self.encoder_model = Model(encoder_input, encoder_states)

        decoder_state_inputs = [Input(shape=(hidden_units,)), Input(shape=(hidden_units,))]
        decoder_outputs, state_h, state_c = decoder_lstm(decoder_input, initial_state=decoder_state_inputs)
        decoder_states = [state_h, state_c]
        decoder_outputs = decoder_dense(decoder_outputs)
        self.decoder_model = Model([decoder_input] + decoder_state_inputs, [decoder_outputs] + decoder_states)
        
        
    def reply(self, input_text):
        """
        Takes input_text and return predicted responce
        :param input_text: string
        :return: predicted_text: string
        """
        if input_text == self.ultimate_question:
            return '42'
        input_seq = []
        input_wids = []
        for word in nltk.word_tokenize(input_text.lower()):
            idx = 1
            if word in input_word2dict:
                idx = input_word2dict[word]
            input_wids.append(idx)
            #pdb.set_trace() break point 1 
        input_seq.append(input_wids)
        #pdb.set_trace() break point 2
        input_seq = pad_sequences(input_seq,encoder_max_seq_length)
        #pdb.set_trace() #break point 3
        states_value = self.encoder_model.predict(input_seq)
        #pdb.set_trace() #4
        target_seq = np.zeros((1, 1, num_decoder_tokens))
        target_seq[0, 0,target_word2dict['START']] = 1
        #pdb.set_trace() #5
        target_text = ''
        target_text_len = 0
        terminated = False
        while not terminated:
            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)
            #pdb.set_trace() #6
            sample_token_idx = np.argmax(output_tokens[0, -1, :])
            #pdb.set_trace() #7
            sample_word = target_dict2word[sample_token_idx]
            #pdb.set_trace()#8
            target_text_len += 1
            #pdb.set_trace() #9
            if sample_word != 'START' and sample_word != 'END':
                target_text += ' ' + sample_word

            if sample_word == 'END' or target_text_len >= decoder_max_seq_length:
                terminated = True

            target_seq = np.zeros((1, 1, num_decoder_tokens))
            target_seq[0, 0, sample_token_idx] = 1

            states_value = [h, c]
        return target_text.strip().replace('UNK', '')


ch=ChatBot()
